################################################################################
# Logstash Pipeline Configuration for PostgreSQL Audit & Connection Logs
# ------------------------------------------------------------------------------
# This pipeline reads PostgreSQL logs, parses both AUDIT and connection events,
# and writes structured data back to PostgreSQL tables via JDBC.
#
# Environment variables (set in docker-compose.yml):
#   - CLUSTER_NAME
#   - SERVER_NAME
#   - SERVER_IP
#   - PG_HOST
#   - PG_PORT
#   - PG_DATABASE
#   - PG_USER
#   - PG_PASSWORD
################################################################################

###################
#  INPUT SECTION  #
###################
input {
  file {
    # Mount your PostgreSQL log directory here
    path              => "/var/lib/pgsql/17/data/log/postgresql-*.log"
    start_position    => "beginning"
    sincedb_path      => "/var/lib/logstash/sincedb/sincedb-combined-logs"
    discover_interval => 2
    stat_interval     => 0.5
    close_older       => 300
    ignore_older      => 0
  }
}

####################
#  FILTER SECTION  #
####################
filter {
  # Add metadata fields (cluster/server info)
  mutate {
    add_field => {
      "cluster_name" => "${CLUSTER_NAME:default_cluster}"
      "server_name"  => "${SERVER_NAME:logstash_node}"
      "server_ip"    => "${SERVER_IP:127.0.0.1}"
    }
  }

  # Parse main PostgreSQL log structure
  grok {
    match => {
      "message" => [
        '^%{TIMESTAMP_ISO8601:log_time}\s+(?<tz>(?:[+-]\d{2}(?::?\d{2})?|UTC))\s+\[%{NUMBER:pid}\]\s+user=%{DATA:username},db=%{DATA:database_name}, client_ip=%{DATA:client_ip}\s+app=%{DATA:application_name}\s+LOG:\s+%{GREEDYDATA:pg_message}$'
      ]
    }
    tag_on_failure => ["_grokparsefailure"]
  }

  # Skip unparsed or irrelevant lines
  if "_grokparsefailure" in [tags] {
    drop { }
  }

  # Combine timestamp and timezone into unified format
  mutate { add_field => { "log_time_full" => "%{log_time} %{tz}" } }

  date {
    match  => ["log_time_full", "YYYY-MM-dd HH:mm:ss.SSS Z"]
    target => "log_time"
    timezone => "UTC"
  }

  # --- Connection Logs ---
  if [pg_message] =~ /^connection received:|^connection authorized:|^disconnection:/ {
    if [pg_message] =~ /^connection received:/ { mutate { add_field => { "action" => "connection_received" } } }
    else if [pg_message] =~ /^connection authorized:/ { mutate { add_field => { "action" => "connection" } } }
    else if [pg_message] =~ /^disconnection:/ { mutate { add_field => { "action" => "disconnection" } } }

    mutate {
      strip => ["username", "database_name", "client_ip", "action", "cluster_name", "server_name", "server_ip", "application_name"]
      remove_field => ["pid", "pg_message", "@version", "host", "event", "log", "message", "log_time_full", "tz"]
    }

    mutate { add_tag => ["connection_log"] }
  }

  # --- Audit Logs ---
  else if [pg_message] =~ /^AUDIT:/ {
    grok {
      match => {
        "pg_message" => "AUDIT:\s+SESSION,%{NUMBER:session_id},%{NUMBER:statement_id},%{WORD:audit_type},%{GREEDYDATA:statement_details}"
      }
    }
    grok {
      match => {
        "statement_details" => "(?<command>[^,]*),(?<object_type>[^,]*),(?<object_name>[^,]*),%{GREEDYDATA:statement_text}"
      }
      overwrite => ["command", "object_type", "object_name", "statement_text"]
    }

    mutate {
      strip => ["username", "session_id", "statement_id", "audit_type", "statement_text",
                "command", "object_type", "object_name", "cluster_name",
                "server_name", "server_ip", "client_ip", "application_name"]
      remove_field => ["pid", "pg_message", "@version", "host", "event", "log", "message", "log_time_full", "tz", "statement_details"]
    }

    mutate { add_tag => ["audit_log"] }
  }

  # Drop anything else
  else {
    drop { }
  }
}

####################
#  OUTPUT SECTION  #
####################
output {
  # --- Connection logs ---
  if "connection_log" in [tags] {
    jdbc {
      connection_string => "jdbc:postgresql://${PG_HOST:postgres}:${PG_PORT:5432}/${PG_DATABASE:auditdb}"
      driver_class      => "org.postgresql.Driver"
      driver_jar_path   => "/usr/share/logstash/logstash-core/lib/jars/postgresql-42.7.8.jar"
      username          => "${PG_USER:postgres}"
      password          => "${PG_PASSWORD:postgres}"
      statement => [
        "INSERT INTO connection_logs (log_time, username, database_name, client_ip, action, cluster_name, server_name, server_ip, application_name)
         VALUES (?::timestamptz, ?, ?, ?, ?, ?, ?, ?, ?)",
        "log_time", "username", "database_name", "client_ip", "action",
        "cluster_name", "server_name", "server_ip", "application_name"
      ]
      flush_size    => 1
      max_pool_size => 5
    }
  }

  # --- Audit logs ---
  if "audit_log" in [tags] {
    jdbc {
      connection_string => "jdbc:postgresql://${PG_HOST:postgres}:${PG_PORT:5432}/${PG_DATABASE:auditdb}"
      driver_class      => "org.postgresql.Driver"
      driver_jar_path   => "/usr/share/logstash/logstash-core/lib/jars/postgresql-42.7.8.jar"
      username          => "${PG_USER:postgres}"
      password          => "${PG_PASSWORD:postgres}"
      statement => [
        "INSERT INTO audit_logs (log_time, username, session_id, statement_id, audit_type, statement_text, command, object_type, object_name, cluster_name, server_name, server_ip, client_ip, application_name)
         VALUES (?::timestamptz, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
        "log_time", "username", "session_id", "statement_id", "audit_type",
        "statement_text", "command", "object_type", "object_name", "cluster_name",
        "server_name", "server_ip", "client_ip", "application_name"
      ]
      flush_size    => 1
      max_pool_size => 5
    }
  }

  # Uncomment for debugging
  # stdout { codec => rubydebug }
}

